\section{Colour Recognition}
Colours have been used by humans in a long time to perceive reality, whether it is through paintings, television, etc. But humans are still not able to express colours in absolute terms, and instead use terms like "London-telephone-box-red" to describe certain nuances of colours, or near the absolute colour with estimations as "the bottle has a green'ish colour".
When working in the field of computer vision, there is the advantage of cameras which can be used as a measurement units and cameras does measurements in absolute quantities. So the "green'ish" colour would be specified with a value which can be replicated exactly.

Colours can be measured as wavelengths. Some colours, such as low frequency ultra violet and high frequency infrared colours, are not part of the human visual spectrum, and therefore requires specialised tools for humans to see. The human visual spectrum is a narrow part of the total colour spectrum, and is approximated to be between 380 nm and 740 nm.
The human visible colours can be represented as a combination of primary colours and there is a number of standardisations for this representation. One such standardisation is the RGB colour space, which used Red, Green and Blue as primary colours, where Red is defined by a wavelength of 700 nm, Green is 546.1 nm, and Blue is 435.8.\citep{obj_recogn_book}

Two physical mechanics \textit{surface reflection} and \textit{body reflection} are the main reasons humans see the colours the way humans do. The shine on some metals is caused by surface reflection. Mirrors make use of this mechanic. The area under the surface of any object, be it animate or inanimate, is defined as the body. Light waves that enters the body may bounce between pigment particles in the body, the pigments then absorb some waves and others are randomly reflected out of the surface again, the reflected waves are the once humans see. Pure black objects are from this definition absorbing all colours, and white is reflecting all colours.

To represent the data needed for images, \textit{palette images} can be used, which additionally reduces the data that needs to be stored, eg. by the use of \textit{n} arrays for \textit{n} colour spaces. The different colours to use are typically represented by 8 bits for each colour. Which gives a total of 256 different values for each colour.

The last aspect of colour to be covered is \textit{colour consistency}, which covers how the same colour can be read differently depending on the level of illumination it is exposed to. The human mind can in some cases abstract and reason about perceived colours, and determine what the true colours are. But cameras cannot do this abstraction, as they only base the colours on the readings they perform. One example could be a Rubik's cube where one side is illuminated and another is shaded. A yellow field on the illuminated side may be interpret as yellow, but a yellow field on the shaded side might be so dark that it is interpreted as orange. To counter this, some cameras have been specialised to read the illumination level of regions of images, and subtract possible illumination levels, to give similar interpretations in all the regions.